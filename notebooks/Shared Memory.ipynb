{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda_driver\n",
    "import pycuda.compiler as cuda_compiler\n",
    "from pycuda.gpuarray import GPUArray\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "import IPythonMagic\n",
    "from Timer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python version 3.6.6 (default, Sep 12 2018, 18:26:19) \n",
      "[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\n"
     ]
    }
   ],
   "source": [
    "%setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering context in user workspace\n",
      "Creating context\n",
      "PyCUDA version 2018.1.1\n",
      "CUDA version (9, 1, 0)\n",
      "Driver version 10000\n",
      "Using 'Tesla K80' GPU\n",
      " => compute capability: (3, 7)\n",
      " => memory: 10392 / 11441 MB available\n",
      "Created context handle <53765520>\n",
      "Using CUDA cache dir /home/ubuntu/jupyter_notebooks/andrea_andreolli/MilanoGPU2018/notebooks/cuda_cache\n"
     ]
    }
   ],
   "source": [
    "%cuda_context_handler context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompileError",
     "evalue": "nvcc compilation of /tmp/tmpvex7b4d0/kernel.cu failed\n[command: nvcc --cubin -arch sm_37 -I/home/ubuntu/.local/lib/python3.6/site-packages/pycuda/cuda kernel.cu]\n[stderr:\nkernel.cu(46): error: expression must be a modifiable lvalue\n\nkernel.cu(7): warning: variable \"gid\" was declared but never referenced\n\n1 error detected in the compilation of \"/tmp/tmpxft_00001204_00000000-6_kernel.cpp1.ii\".\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCompileError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f8446c3fd963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \"\"\"\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mkernel_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda_compiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSourceModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_src\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mkernel_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shMemReduction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pycuda/compiler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         cubin = compile(source, nvcc, options, keep, no_extern_c,\n\u001b[0;32m--> 291\u001b[0;31m                 arch, code, cache_dir, include_dirs)\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpycuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_from_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pycuda/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs, target)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-I\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompile_plain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnvcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pycuda/compiler.py\u001b[0m in \u001b[0;36mcompile_plain\u001b[0;34m(source, options, keep, nvcc, cache_dir, target)\u001b[0m\n\u001b[1;32m    135\u001b[0m         raise CompileError(\"nvcc compilation of %s failed\" % cu_file_path,\n\u001b[1;32m    136\u001b[0m                 \u001b[0mcmdline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 stderr=stderr.decode(\"utf-8\", \"replace\"))\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstdout\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCompileError\u001b[0m: nvcc compilation of /tmp/tmpvex7b4d0/kernel.cu failed\n[command: nvcc --cubin -arch sm_37 -I/home/ubuntu/.local/lib/python3.6/site-packages/pycuda/cuda kernel.cu]\n[stderr:\nkernel.cu(46): error: expression must be a modifiable lvalue\n\nkernel.cu(7): warning: variable \"gid\" was declared but never referenced\n\n1 error detected in the compilation of \"/tmp/tmpxft_00001204_00000000-6_kernel.cpp1.ii\".\n]"
     ]
    }
   ],
   "source": [
    "kernel_src = \"\"\"\n",
    "\n",
    "__global__ void shMemReduction(float* output, float* input, int size) {\n",
    "    \n",
    "    // First we stride thorugh global memory and compute the maximum for every thread\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x; //blockId.x is always zero because I use one block only\n",
    "    \n",
    "    // I didn't really get the trick here, the point is that this code accesses global GPU memory efficiently\n",
    "    float max_value = -99999999.99; // FIX ME WITH CORRECT VALUE\n",
    "    for(int i = threadIdx.x; i < size; i = i + blockDim.x) {\n",
    "        max_value = fmaxf(max_value, input[i]);\n",
    "    }\n",
    "    \n",
    "    // Temporary write to memory to check if things work so far\n",
    "    output[threadIdx.x] = max_value;\n",
    "    \n",
    "    // Store the per-thread maximum in shared memory\n",
    "    __shared__ float max_shared[32];\n",
    "    max_shared[threadIdx.x] = max_value;\n",
    "    \n",
    "    // Synchronise so that all thread see the same memory\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Find the maximum in memory: 32 to 16\n",
    "    if (threadIdx.x < 16) {\n",
    "         max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 16]);\n",
    "    }\n",
    "    \n",
    "    // Find the maximum in memory: 16 to 8\n",
    "    if (threadIdx.x < 8) {\n",
    "         max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 16]);\n",
    "    }\n",
    "   \n",
    "   // Find the maximum in memory: 8 to 4\n",
    "    if (threadIdx.x < 4) {\n",
    "         max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 16]);\n",
    "    }\n",
    "    \n",
    "    // Find the maximum in memory: 4 to 2\n",
    "    if (threadIdx.x < 2) {\n",
    "         max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 16]);\n",
    "    }\n",
    "    \n",
    "    // Find the maximum in memory\n",
    "    if (threadIdx.x = 0) {\n",
    "         max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 16]);\n",
    "         output[0] = max_shared[0];\n",
    "    }\n",
    "    \n",
    "    // Finally write output\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "kernel_module = cuda_compiler.SourceModule(kernel_src)\n",
    "kernel_function = kernel_module.get_function(\"shMemReduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05676738 0.88570774 0.8356549  0.04014112 0.20502482 0.73145306\n",
      "  0.46846572 0.07788982 0.0980249  0.19718513 0.12497357 0.54839116\n",
      "  0.44450334 0.54450774 0.06586034 0.01848972 0.09302702 0.9707608\n",
      "  0.6441077  0.18992673 0.70981276 0.6398528  0.8103699  0.51682216\n",
      "  0.4633676  0.39644143 0.5802235  0.769677   0.71685123 0.29946277\n",
      "  0.22802688 0.9052712  0.9620451  0.6985464  0.61725545 0.23405337\n",
      "  0.9067984  0.7051695  0.46646544 0.01990609 0.6253213  0.35327104\n",
      "  0.33566865 0.96801126 0.4930374  0.5540883  0.60538465 0.04287912\n",
      "  0.47265834 0.58728296 0.21084785 0.6005624  0.9926177  0.35323107\n",
      "  0.2935759  0.51132464 0.2672205  0.17414628 0.362874   0.34549886\n",
      "  0.67795324 0.18047003 0.72376376 0.01169576]]\n"
     ]
    }
   ],
   "source": [
    "n = 64\n",
    "a = np.random.random((1,n)).astype(np.float32)\n",
    "print(a)\n",
    "num_threads = 32\n",
    "b = np.empty((1,num_threads))\n",
    "\n",
    "a_g = GPUArray(a.shape,a.dtype)\n",
    "a_g.set(a)\n",
    "b_g = GPUArray(b.shape,b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05676738 0.88570774 0.8356549  0.04014112 0.20502482 0.73145306\n",
      "  0.46846572 0.07788982 0.0980249  0.19718513 0.12497357 0.54839116\n",
      "  0.44450334 0.54450774 0.06586034 0.01848972 0.09302702 0.9707608\n",
      "  0.6441077  0.18992673 0.70981276 0.6398528  0.8103699  0.51682216\n",
      "  0.4633676  0.39644143 0.5802235  0.769677   0.71685123 0.29946277\n",
      "  0.22802688 0.9052712  0.9620451  0.6985464  0.61725545 0.23405337\n",
      "  0.9067984  0.7051695  0.46646544 0.01990609 0.6253213  0.35327104\n",
      "  0.33566865 0.96801126 0.4930374  0.5540883  0.60538465 0.04287912\n",
      "  0.47265834 0.58728296 0.21084785 0.6005624  0.9926177  0.35323107\n",
      "  0.2935759  0.51132464 0.2672205  0.17414628 0.362874   0.34549886\n",
      "  0.67795324 0.18047003 0.72376376 0.01169576]]\n",
      "[[2.28774221e-03 5.89911976e-08 4.15832328e-04 7.16644537e-12\n",
      "  1.24423354e-06 5.81320466e-03 5.69278853e-05 5.61912508e-14\n",
      "  5.98505231e-03 9.82054769e-05 1.51079759e-04 3.87315303e-05\n",
      "  3.21603010e-06 6.42007797e-04 3.77371008e-07 2.89909960e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "block_size = (num_threads, 1, 1)\n",
    "grid_size = (1,1,1)\n",
    "\n",
    "kernel_function(b_g,a_g, np.int32(n), grid=grid_size, block=block_size)\n",
    "\n",
    "b_g.get(b)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
